{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SemCom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\leona\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "#import torchtext; #requires downgrading torch\n",
    "#pip install torch==2.2.2\n",
    "#pip install torchtext==0.17.2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import tf_keras\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_21068\\3954474801.py:22: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from numpy import trapz\n",
    "from scipy import spatial, stats\n",
    "from scipy.integrate import simpson\n",
    "from scipy.interpolate import splrep, BSpline\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S Set of Data Points: MNIST digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(S):\n",
    "  scaler = MinMaxScaler(feature_range = (0,1))\n",
    "  return scaler.fit_transform(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', as_frame=False, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = mnist.data.astype('float32')\n",
    "S_labels = mnist.target.astype('int64')\n",
    "df_S_labels = pd.DataFrame(data=S_labels, columns=[\"S_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "print(len(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_digit_array(digit):\n",
    "    for pixels_row in digit.reshape(28,28):\n",
    "        print(np.array2string(pixels_row).replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 159. 253. 159.  50.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  48. 238. 252. 252. 252. 237.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  54. 227. 253. 252. 239. 233. 252.  57.   6.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  10.  60. 224. 252. 253. 252. 202.  84. 252. 253. 122.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 163. 252. 252. 252. 253. 252. 252.  96. 189. 253. 167.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 238. 253. 253. 190. 114. 253. 228.  47.  79. 255. 168.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.  48. 238. 252. 252. 179.  12.  75. 121.  21.   0.   0. 253. 243.  50.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.  38. 165. 253. 233. 208.  84.   0.   0.   0.   0.   0.   0. 253. 252. 165.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   7. 178. 252. 240.  71.  19.  28.   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.  57. 252. 252.  63.   0.   0.   0.   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0. 198. 253. 190.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 255. 253. 196.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  76. 246. 252. 112.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 253. 252. 148.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  85. 252. 230.  25.   0.   0.   0.   0.   0.   0.   0.   0.   7. 135. 253. 186.  12.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  85. 252. 223.   0.   0.   0.   0.   0.   0.   0.   0.   7. 131. 252. 225.  71.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  85. 252. 145.   0.   0.   0.   0.   0.   0.   0.  48. 165. 252. 173.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  86. 253. 225.   0.   0.   0.   0.   0.   0. 114. 238. 253. 162.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  85. 252. 249. 146.  48.  29.  85. 178. 225. 253. 223. 167.  56.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  85. 252. 252. 252. 229. 215. 252. 252. 252. 196. 130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.  28. 199. 252. 252. 253. 252. 252. 233. 145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.  25. 128. 252. 253. 252. 141.  37.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Second data point is an image of a zero\n",
    "print_digit_array(S[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CIFAR10*\n",
    "\n",
    "We can continue with a more complex dataset if needed.\n",
    "\n",
    "E.g., CIFAR10 is an image dataset on 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA-based Encoder-Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_pca_variance(X, v):\n",
    "    '''\n",
    "    v: explained_variance_threshold\n",
    "    '''\n",
    "    # Center the data\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X_centered = X - mean\n",
    "\n",
    "    # Compute SVD\n",
    "    _, singular_values, all_pca = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "    # Compute the squared singular values for explained variance\n",
    "    singular_values_squared = singular_values ** 2\n",
    "\n",
    "\n",
    "    # Compute the explained variance for each principal component\n",
    "    explained_variance = singular_values_squared / np.sum(singular_values_squared)\n",
    "\n",
    "    # Compute cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "    # Determine the number of components required to reach the threshold\n",
    "    n_components = np.searchsorted(cumulative_explained_variance, v) + 1\n",
    "\n",
    "    # Select the top n_components\n",
    "    pca = all_pca[:n_components, :] #Principal Components (Vt)\n",
    "\n",
    "    return mean, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_encode(X, mean, pca):\n",
    "    #Encoder\n",
    "    # Center the new dataset with the same mean vector\n",
    "    X_centered = X - mean\n",
    "    # Transform the data using the principal components\n",
    "    X_encoded = X_centered @ pca.T\n",
    "\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_decode(X_encoded, mean, pca):\n",
    "    #Decoder\n",
    "    # Reconstruct the data\n",
    "    X_decoded = X_encoded @ pca\n",
    "    # Reverse centering by adding the mean vector\n",
    "    X_decoded_centered = X_decoded + mean    \n",
    "    return X_decoded_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_encode_decode_reconstruction_error(X, mean, pca):\n",
    "    #Encoder\n",
    "    X_centered = X - mean\n",
    "    # Transform data (compression)\n",
    "    X_encoded = X_centered @ pca.T\n",
    "\n",
    "    #Decoder\n",
    "    # Reconstruct the data\n",
    "    X_decoded = X_encoded @ pca #@ = np.dot = matrix multiplication\n",
    "    # Reverse centering by adding the mean vector\n",
    "    X_decoded_centered = X_decoded + mean\n",
    "\n",
    "    #Compute mean squared error between original and decompressed data\n",
    "    mse = pd.Series(map(mean_squared_error, X, X_decoded_centered))\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Self-Supervised Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, pca = method_pca_variance(X=S, v=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding (Compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_z = pca_encode(S, mean, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1010.494    , -289.9636   , -576.1207   , -485.08362  ,\n",
       "        841.47833  , -145.46716  ,  -48.333355 ,  102.231064 ,\n",
       "        -34.786186 , -151.26501  , -282.70004  , -110.107765 ,\n",
       "        329.9303   ,  -97.00281  , -226.4773   , -156.05844  ,\n",
       "        -49.416885 ,  -37.92279  ,   63.192932 ,  113.47844  ,\n",
       "        202.56096  ,  -94.196815 ,  -12.712128 ,  -29.628128 ,\n",
       "        233.85992  ,  196.91718  ,   18.939308 ,  -28.379498 ,\n",
       "        363.63416  ,  -84.49492  ,   -7.011917 , -100.57101  ,\n",
       "        192.7258   ,   86.99288  , -137.27632  ,  -79.502464 ,\n",
       "         61.11911  ,    4.0909624,  -43.707153 ,  -83.74587  ,\n",
       "        219.2553   ,  -64.80328  ,   68.30545  ], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compressed second digit\n",
    "S_z[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size of a digit:\n",
      "784\n",
      "Compressed size of a digit:\n",
      "43\n",
      "Reduction:\n",
      "94.52%\n",
      "-------\n",
      "Original size of a digit times dataset size:\n",
      "784 x 70000 = 54880000\n",
      "Compressed size of a digit times dataset size:\n",
      "43 x 70000 = 3010000\n"
     ]
    }
   ],
   "source": [
    "print(\"Original size of a digit:\")\n",
    "print(len(S[0]))\n",
    "print(\"Compressed size of a digit:\")\n",
    "print(len(S_z[0]))\n",
    "print(\"Reduction:\")\n",
    "print(f\"{1 - (len(S_z[0])/len(S[0])):.2%}\")\n",
    "\n",
    "print(\"-------\")\n",
    "print(\"Original size of a digit times dataset size:\")\n",
    "print(str(len(S[0])) + \" x \" + str(len(S)) + \" = \" + str(len(S[0])*len(S)))\n",
    "print(\"Compressed size of a digit times dataset size:\")\n",
    "print(str(len(S_z[0])) + \" x \" + str(len(S)) + \" = \" + str(len(S_z[0])*len(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoding (Reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_hat = pca_decode(S_z, mean, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "[  0   0   0   0   0   0   0   0  -1  -1  -2  -4  -7 -10 -10  -6   3  10  13  10   6   2   0   0   0   0   0   0]\n",
      "[  0   0   0   0   0   0  -1  -2  -4  -7 -10 -16 -22 -26 -23  -7  16  30  31  21  10   3   0   0   0   0   0   0]\n",
      "[  0   0   0   0   0  -1  -2  -6 -10 -15 -21 -29 -33 -20  23  95 159 179 143  85  32   5  -2  -3  -1   0   0   0]\n",
      "[  0   0   0   0   0  -1  -3  -6  -9 -14 -23 -33 -29   8  85 182 262 282 232 141  54   2 -13 -11  -4  -1   0   0]\n",
      "[  0   0   0   0   1   2   4   5   3  -4 -17 -20   9  73 147 212 253 267 242 169  83  18 -10 -12  -4   0   0   0]\n",
      "[  0   0   0   1   4   9  14  18  13   0  -5  27  99 181 230 239 231 223 213 187 131  69  26   6   4   3   1   0]\n",
      "[  0   0   1   2   6  11  14  12   2  -5  27 113 215 276 277 245 207 172 164 179 175 133  75  32  12   5   1   0]\n",
      "[  0   0   0   2   4   4   0 -11 -19   9  96 205 275 270 222 180 144 104  86 131 184 175 114  51  14   4   1   0]\n",
      "[  0   0   0   0   0  -4 -17 -28  -7  75 182 248 234 164 106  83  72  31  10  81 180 199 138  62  14   2   1   0]\n",
      "[  0   0   0   0  -2  -9 -23 -14  57 174 240 213 124  42  17  28  36  -8 -25  62 184 216 151  67  15   0   0   0]\n",
      "[  0   0   0   0  -2  -9 -14  32 148 249 232 117  11 -31 -10  25  31 -23 -31  68 196 225 155  67  14   0   0   0]\n",
      "[  0   0   0   0  -1  -5  12  95 220 263 161  14 -59 -43  11  50  23 -39 -32  80 206 229 150  62  11  -1   0   0]\n",
      "[  0   0   0   0   0   6  48 150 245 218  71 -56 -74 -13  45  57  -1 -57 -31  95 215 224 138  51   6  -1   0   0]\n",
      "[  0   0   0   0   4  24  85 180 231 154   6 -75 -44  21  54  33 -23 -52  -3 119 218 207 114  34   0  -2   0   0]\n",
      "[  0   0   0   0   9  46 113 193 206 111 -14 -47  -1  29  22  -5 -17  -2  59 154 209 168  77  14  -6  -3   0   0]\n",
      "[  0   0   0   0  16  63 129 194 187  93  -7 -23  -1  -7 -23 -12  38  92 137 172 170 105  33  -3  -9  -4  -1   0]\n",
      "[  0   0   0   0  21  72 136 190 178  91   1 -29 -31 -38 -22  44 137 187 180 148  97  38   0 -11  -9  -4   0   0]\n",
      "[  0   0   0   0  23  76 138 189 181 107  21 -23 -23   4  61 152 227 224 156  77  23  -3 -11 -11  -7  -3  -1   0]\n",
      "[  0   0   0   1  20  71 137 193 205 163  99  65  81 126 181 228 229 165  70   8  -8  -9  -7  -6  -4  -2   0   0]\n",
      "[  0   0   0   1  14  55 121 193 240 248 233 224 234 242 228 192 133  59   6  -8  -3   0  -1  -2  -2  -1   0   0]\n",
      "[  0   0   0   1   7  31  82 156 230 281 302 303 283 229 150  73  22   1   0   5   8   5   1  -1  -1   0   0   0]\n",
      "[  0   0   0   1   3  10  33  77 141 204 244 241 187  96   8 -34 -23   2  14  13   9   4   1   0   0   0   0   0]\n",
      "[  0   0   0   0   1   2   5  14  29  48  61  53  16 -27 -50 -36  -4  16  16  10   4   1   0   0   0   0   0   0]\n",
      "[  0   0   0   0   0   1   2   4  10  17  22  18   3 -13 -17  -7   6  12  10   5   2   0   0   0   0   0   0   0]\n",
      "[ 0  0  0  0  0  0  1  3  6 11 13 11  4 -3 -4 -1  4  5  4  2  1  0  0  0  0  0  0  0]\n",
      "[ 0  0  0  0  0  0  0  0  0  1  1  0  0 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print_digit_array(np.array([round(pixel) for pixel in S_hat[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_accuracy = pca_encode_decode_reconstruction_error(S, mean, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1066.612061\n",
       "1         860.795349\n",
       "2        1742.022949\n",
       "3         378.988495\n",
       "4         943.702026\n",
       "            ...     \n",
       "69995     944.032043\n",
       "69996     611.196838\n",
       "69997     522.794312\n",
       "69998    1120.114258\n",
       "69999    1277.763428\n",
       "Length: 70000, dtype: float32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
